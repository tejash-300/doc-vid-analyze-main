{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133049,"status":"ok","timestamp":1741254242990,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"BUjqU2N_ynyB","outputId":"b56178d0-fd27-4df2-8ac0-4d8f8b2fb7a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Collecting fastapi\n","  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n","Collecting uvicorn\n","  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n","Collecting pdfplumber\n","  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n","\u001b[0mCollecting openai-whisper\n","  Downloading openai-whisper-20240930.tar.gz (800 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.61.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.5.1+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n","Collecting tiktoken (from openai-whisper)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.17.0)\n","Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.44.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803373 sha256=c8e38669e5b1485847e7ce4e85b5ff4442c3d6b1e7581a220f19579dcb043ee8\n","  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.28.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n","Collecting pydantic==1.10.8\n","  Downloading pydantic-1.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (146 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==1.10.8) (4.12.2)\n","Downloading pydantic-1.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydantic\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.10.6\n","    Uninstalling pydantic-2.10.6:\n","      Successfully uninstalled pydantic-2.10.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","wandb 0.19.7 requires pydantic<3,>=2.6, but you have pydantic 1.10.8 which is incompatible.\n","google-genai 1.2.0 requires pydantic<3.0.0dev,>=2.0.0, but you have pydantic 1.10.8 which is incompatible.\n","langchain 0.3.19 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.8 which is incompatible.\n","albumentations 2.0.4 requires pydantic>=2.9.2, but you have pydantic 1.10.8 which is incompatible.\n","langchain-core 0.3.40 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pydantic-1.10.8\n"]}],"source":["!pip install torch fastapi uvicorn spacy pdfplumber moviepy librosa soundfile matplotlib numpy json tempfile transformers sentence-transformers pyngrok\n","!pip install openai-whisper\n","!pip install accelerate\n","!pip install pydantic==1.10.8  # Downgrade if needed for FastAPI compatibility\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19172,"status":"ok","timestamp":1741254262170,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"pxv6a38YyujN","outputId":"09c87858-fc53-4653-9363-2302a31ab1b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.8)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!python -m spacy download en_core_web_sm\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4013,"status":"ok","timestamp":1741254266189,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"W0rgVGoT18dD","outputId":"044afe32-b079-41eb-b7f4-a804b0512020"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.2.3\n"]}],"source":["!pip install pyngrok\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2995,"status":"ok","timestamp":1741254269186,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"xsuFKfo6zOqZ","outputId":"628863a3-c3b7-447f-f919-7612906132bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}],"source":["!ngrok authtoken 2tcXhKrQ46IoUz9QKx82ppnTLT5_27c6sMjdwa2KPAAPGyyMx"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2331,"status":"ok","timestamp":1741254336097,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"oqZTGLP10LrR","outputId":"659a4819-af57-4d90-ec2c-438a58a4ab33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting uvicorn\n","  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n","Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: uvicorn\n","Successfully installed uvicorn-0.34.0\n"]}],"source":["!pip install uvicorn\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4837,"status":"ok","timestamp":1741254343543,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"uurm9_V30m-F","outputId":"2d5b8b2e-6422-48de-db44-5f52ebbc1062"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n","Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n","Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n","Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"]}],"source":["!pip install pdfplumber\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3504,"status":"ok","timestamp":1741254347049,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"Yt9E4f741diR","outputId":"bf04bd32-3d91-49ed-b96c-b3d37ff374d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.11)\n","Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (1.10.8)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n","Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (3.7.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n"]}],"source":["!pip install fastapi\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3448,"status":"ok","timestamp":1741254353534,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"Cr-yY8Jj6rWa","outputId":"a6f6526e-118d-4be0-e11a-185cb53f4d6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n"]}],"source":["!pip install python-multipart\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1741254353564,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"},"user_tz":-330},"id":"24m_ZfAjxtc_","outputId":"1ca3dd6f-c278-4b45-c264-112c52283d2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}],"source":["%%writefile app.py\n","\n","import os\n","import io\n","import torch\n","import uvicorn\n","import spacy\n","import pdfplumber\n","import moviepy.editor as mp\n","import librosa\n","import soundfile as sf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import json\n","import tempfile\n","from fastapi import FastAPI, UploadFile, File, HTTPException, Form\n","from fastapi.responses import FileResponse, JSONResponse\n","from fastapi.middleware.cors import CORSMiddleware\n","from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n","from sentence_transformers import SentenceTransformer\n","from pyngrok import ngrok\n","from threading import Thread\n","import time\n","import uuid\n","\n","# ✅ Ensure compatibility with Google Colab\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","except:\n","    pass  # Skip drive mount if not in Google Colab\n","\n","# ✅ Ensure required directories exist\n","os.makedirs(\"static\", exist_ok=True)\n","os.makedirs(\"temp\", exist_ok=True)\n","\n","# ✅ Ensure GPU usage\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# ✅ Initialize FastAPI\n","app = FastAPI(title=\"Legal Document and Video Analyzer\")\n","\n","# Add CORS middleware\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","# ✅ Initialize document storage\n","document_storage = {}\n","chat_history = []  # ✅ Added global chat history\n","\n","# ✅ Function to store document context by task ID\n","def store_document_context(task_id, text):\n","    \"\"\"Store document text for retrieval by chatbot.\"\"\"\n","    document_storage[task_id] = text\n","    return True\n","\n","# ✅ Function to load document context by task ID\n","def load_document_context(task_id):\n","    \"\"\"Retrieve document text for chatbot context.\"\"\"\n","    return document_storage.get(task_id, \"\")\n","\n","# ✅ Load NLP Models\n","try:\n","    # Download spacy model if not available\n","    try:\n","        nlp = spacy.load(\"en_core_web_sm\")\n","    except:\n","        spacy.cli.download(\"en_core_web_sm\")\n","        nlp = spacy.load(\"en_core_web_sm\")\n","\n","    print(\"✅ Loading NLP models...\")\n","\n","    # Use device_map=\"auto\" for better GPU utilization\n","    summarizer = pipeline(\"summarization\", model=\"nsi319/legal-pegasus\",\n","                      device=0 if torch.cuda.is_available() else -1)\n","\n","    embedding_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n","\n","    ner_model = pipeline(\"ner\", model=\"dslim/bert-base-NER\",\n","                     device=0 if torch.cuda.is_available() else -1)\n","\n","    # For video analysis - use a more robust speech recognition model\n","    speech_to_text = pipeline(\"automatic-speech-recognition\",\n","                             model=\"openai/whisper-medium\",\n","                             chunk_length_s=30,\n","                             device_map=\"auto\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # ✅ Load CUAD Clause Classification Model\n","    cuad_tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n","    cuad_model = AutoModelForSequenceClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n","    cuad_model.to(device)\n","\n","    print(\"✅ All models loaded successfully\")\n","\n","except Exception as e:\n","    print(f\"⚠️ Error loading models: {str(e)}\")\n","    raise RuntimeError(f\"Error loading models: {str(e)}\")\n","\n","from transformers import pipeline\n","\n","# ✅ Load a Question Answering Model\n","qa_model = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n","\n","def legal_chatbot(user_input, context):\n","    \"\"\"Uses a real NLP model for legal Q&A.\"\"\"\n","    global chat_history  # ✅ Use global chat history\n","\n","    # Append user question to chat history\n","    chat_history.append({\"role\": \"user\", \"content\": user_input})\n","\n","    # Get the AI-based response\n","    response = qa_model(question=user_input, context=context)[\"answer\"]\n","\n","    # Append response to chat history\n","    chat_history.append({\"role\": \"assistant\", \"content\": response})\n","    return response\n","\n","# ✅ PDF Text Extraction\n","def extract_text_from_pdf(pdf_file):\n","    \"\"\"Extracts text from a PDF file using pdfplumber.\"\"\"\n","    try:\n","        with pdfplumber.open(pdf_file) as pdf:\n","            text = \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n","        return text.strip() if text else None\n","    except Exception as e:\n","        raise HTTPException(status_code=400, detail=f\"PDF extraction failed: {str(e)}\")\n","\n","# ✅ Video Processing - Extract Audio and Transcribe\n","def process_video_to_text(video_file_path):\n","    \"\"\"Extract audio from video and convert to text.\"\"\"\n","    try:\n","        print(f\"Processing video file at {video_file_path}\")\n","\n","        # Create temp directory for audio extraction\n","        temp_audio_path = os.path.join(\"temp\", \"extracted_audio.wav\")\n","\n","        # Extract audio from video\n","        video = mp.VideoFileClip(video_file_path)\n","        video.audio.write_audiofile(temp_audio_path, codec='pcm_s16le')\n","\n","        print(f\"Audio extracted to {temp_audio_path}\")\n","\n","        # Load audio for transcription\n","        # Process in chunks to handle large files\n","        full_transcript = \"\"\n","\n","        # Process using whisper pipeline\n","        result = speech_to_text(temp_audio_path)\n","        transcript = result[\"text\"]\n","\n","        print(f\"Transcription completed: {len(transcript)} characters\")\n","\n","        # Clean up\n","        if os.path.exists(temp_audio_path):\n","            os.remove(temp_audio_path)\n","\n","        return transcript\n","    except Exception as e:\n","        print(f\"Error in video processing: {str(e)}\")\n","        raise HTTPException(status_code=400, detail=f\"Video processing failed: {str(e)}\")\n","\n","# ✅ Audio Processing - Direct Audio Transcription\n","def process_audio_to_text(audio_file_path):\n","    \"\"\"Process audio file and convert to text.\"\"\"\n","    try:\n","        print(f\"Processing audio file at {audio_file_path}\")\n","\n","        # Process using whisper pipeline\n","        result = speech_to_text(audio_file_path)\n","        transcript = result[\"text\"]\n","\n","        print(f\"Transcription completed: {len(transcript)} characters\")\n","\n","        return transcript\n","    except Exception as e:\n","        print(f\"Error in audio processing: {str(e)}\")\n","        raise HTTPException(status_code=400, detail=f\"Audio processing failed: {str(e)}\")\n","\n","# ✅ Named Entity Recognition (NER)\n","def extract_named_entities(text):\n","    \"\"\"Extracts named entities from legal text.\"\"\"\n","    # Process text in chunks to avoid memory issues\n","    max_length = 10000  # Process 10K characters at a time\n","    entities = []\n","\n","    for i in range(0, len(text), max_length):\n","        chunk = text[i:i+max_length]\n","        doc = nlp(chunk)\n","        entities.extend([{\"entity\": ent.text, \"label\": ent.label_} for ent in doc.ents])\n","\n","    return entities\n","\n","# ✅ Legal Risk Assessment (Original Function)\n","def analyze_risk(text):\n","    \"\"\"Analyzes legal risk in the document using keyword-based analysis.\"\"\"\n","    risk_keywords = {\n","        \"Liability\": [\"liability\", \"responsible\", \"responsibility\", \"legal obligation\"],\n","        \"Termination\": [\"termination\", \"breach\", \"contract end\", \"default\"],\n","        \"Indemnification\": [\"indemnification\", \"indemnify\", \"hold harmless\", \"compensate\", \"compensation\"],\n","        \"Payment Risk\": [\"payment\", \"terms\", \"reimbursement\", \"fee\", \"schedule\", \"invoice\", \"money\"],\n","        \"Insurance\": [\"insurance\", \"coverage\", \"policy\", \"claims\"],\n","    }\n","    risk_scores = {category: 0 for category in risk_keywords}\n","    lower_text = text.lower()\n","    for category, keywords in risk_keywords.items():\n","        for keyword in keywords:\n","            risk_scores[category] += lower_text.count(keyword.lower())\n","    return risk_scores\n","\n","# ✅ New: Contextual Extraction for Risk Terms\n","def extract_context_for_risk_terms(text, risk_keywords, window=1):\n","    \"\"\"\n","    Extracts and summarizes the context around risk terms.\n","\n","    Parameters:\n","        text (str): The full text of the document.\n","        risk_keywords (dict): A dictionary with risk categories and their keyword lists.\n","        window (int): Number of sentences before and after the risk occurrence to include.\n","\n","    Returns:\n","        dict: Mapping risk categories to their summarized contextual details.\n","    \"\"\"\n","    # Process the text with spaCy for sentence segmentation\n","    doc = nlp(text)\n","    sentences = list(doc.sents)\n","\n","    # Dictionary to collect contexts for each risk category\n","    risk_contexts = {category: [] for category in risk_keywords}\n","\n","    # Loop over sentences and find risk term occurrences\n","    for i, sent in enumerate(sentences):\n","        sent_text_lower = sent.text.lower()\n","        for category, details in risk_keywords.items():\n","            for keyword in details[\"keywords\"]:\n","                if keyword.lower() in sent_text_lower:\n","                    # Define a window around the current sentence\n","                    start_idx = max(0, i - window)\n","                    end_idx = min(len(sentences), i + window + 1)\n","                    context_chunk = \" \".join([s.text for s in sentences[start_idx:end_idx]])\n","                    risk_contexts[category].append(context_chunk)\n","\n","    # Summarize the collected contexts for each risk category using the summarizer model\n","    summarized_contexts = {}\n","    for category, contexts in risk_contexts.items():\n","        if contexts:\n","            combined_context = \" \".join(contexts)\n","            try:\n","                summary_result = summarizer(combined_context, max_length=100, min_length=30, do_sample=False)\n","                summary = summary_result[0]['summary_text']\n","            except Exception as e:\n","                summary = \"Context summarization failed.\"\n","            summarized_contexts[category] = summary\n","        else:\n","            summarized_contexts[category] = \"No contextual details found.\"\n","\n","    return summarized_contexts\n","\n","def get_detailed_risk_info(text):\n","    \"\"\"\n","    Returns detailed risk information by merging risk scores with descriptive details\n","    and contextual summaries from the document.\n","    \"\"\"\n","    # Detailed risk information dictionary\n","    risk_details = {\n","        \"Liability\": {\n","            \"description\": \"Liability refers to the legal responsibility for losses or damages.\",\n","            \"common_concerns\": \"Broad liability clauses may expose parties to unforeseen risks.\",\n","            \"recommendations\": \"Review and negotiate clear limits on liability.\",\n","            \"example\": \"E.g., 'The party shall be liable for direct damages due to negligence.'\"\n","        },\n","        \"Termination\": {\n","            \"description\": \"Termination involves conditions under which a contract can be ended.\",\n","            \"common_concerns\": \"Unilateral termination rights or ambiguous conditions can be risky.\",\n","            \"recommendations\": \"Ensure termination clauses are balanced and include notice periods.\",\n","            \"example\": \"E.g., 'Either party may terminate the agreement with 30 days notice.'\"\n","        },\n","        \"Indemnification\": {\n","            \"description\": \"Indemnification requires one party to compensate for losses incurred by the other.\",\n","            \"common_concerns\": \"Overly broad indemnification can shift significant risk.\",\n","            \"recommendations\": \"Negotiate clear limits and carve-outs where necessary.\",\n","            \"example\": \"E.g., 'The seller shall indemnify the buyer against claims from product defects.'\"\n","        },\n","        \"Payment Risk\": {\n","            \"description\": \"Payment risk pertains to terms regarding fees, schedules, and reimbursements.\",\n","            \"common_concerns\": \"Vague payment terms or hidden charges increase risk.\",\n","            \"recommendations\": \"Clarify payment conditions and include penalties for delays.\",\n","            \"example\": \"E.g., 'Payments must be made within 30 days, with a 2% late fee thereafter.'\"\n","        },\n","        \"Insurance\": {\n","            \"description\": \"Insurance risk covers the adequacy and scope of required coverage.\",\n","            \"common_concerns\": \"Insufficient insurance can leave parties exposed in unexpected events.\",\n","            \"recommendations\": \"Review insurance requirements to ensure they meet the risk profile.\",\n","            \"example\": \"E.g., 'The contractor must maintain liability insurance with at least $1M coverage.'\"\n","        }\n","    }\n","\n","    # Get basic risk scores from the original function\n","    risk_scores = analyze_risk(text)\n","\n","    # Define risk keywords for context extraction\n","    risk_keywords_context = {\n","        \"Liability\": {\"keywords\": [\"liability\", \"responsible\", \"responsibility\", \"legal obligation\"]},\n","        \"Termination\": {\"keywords\": [\"termination\", \"breach\", \"contract end\", \"default\"]},\n","        \"Indemnification\": {\"keywords\": [\"indemnification\", \"indemnify\", \"hold harmless\", \"compensate\", \"compensation\"]},\n","        \"Payment Risk\": {\"keywords\": [\"payment\", \"terms\", \"reimbursement\", \"fee\", \"schedule\", \"invoice\", \"money\"]},\n","        \"Insurance\": {\"keywords\": [\"insurance\", \"coverage\", \"policy\", \"claims\"]}\n","    }\n","\n","    # Extract summarized contextual details for each risk term\n","    risk_contexts = extract_context_for_risk_terms(text, risk_keywords_context, window=1)\n","\n","    detailed_info = {}\n","    for risk_term, score in risk_scores.items():\n","        if score > 0:\n","            info = risk_details.get(risk_term, {\"description\": \"No details available.\"})\n","            detailed_info[risk_term] = {\n","                \"score\": score,\n","                \"description\": info.get(\"description\", \"\"),\n","                \"common_concerns\": info.get(\"common_concerns\", \"\"),\n","                \"recommendations\": info.get(\"recommendations\", \"\"),\n","                \"example\": info.get(\"example\", \"\"),\n","                \"context_summary\": risk_contexts.get(risk_term, \"No context available.\")\n","            }\n","    return detailed_info\n","\n","# ✅ Clause Classification (CUAD)\n","def analyze_contract_clauses(text):\n","    \"\"\"Analyzes contract clauses using CUAD (Contract Understanding Atticus Dataset).\"\"\"\n","    # Process text in chunks to handle large documents\n","    max_length = 512\n","    step = 256  # Overlap to catch clauses that might span chunk boundaries\n","    clauses_detected = []\n","\n","    clause_types = [\n","        \"Obligations of Seller\", \"Governing Law\", \"Termination\", \"Indemnification\",\n","        \"Confidentiality\", \"Insurance\", \"Non-Compete\", \"Change of Control\",\n","        \"Assignment\", \"Warranty\", \"Limitation of Liability\", \"Arbitration\",\n","        \"IP Rights\", \"Force Majeure\", \"Revenue/Profit Sharing\", \"Audit Rights\"\n","    ]\n","\n","    # Split text into manageable chunks\n","    chunks = [text[i:i+max_length] for i in range(0, len(text), step) if i+step < len(text)]\n","\n","    # Process each chunk\n","    for chunk in chunks:\n","        inputs = cuad_tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n","        with torch.no_grad():\n","            outputs = cuad_model(**inputs)\n","\n","        predictions = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n","\n","        for idx, confidence in enumerate(predictions):\n","            if confidence > 0.5 and idx < len(clause_types):\n","                clauses_detected.append({\"type\": clause_types[idx], \"confidence\": float(confidence)})\n","\n","    # Aggregate duplicate clauses by taking highest confidence\n","    aggregated_clauses = {}\n","    for clause in clauses_detected:\n","        clause_type = clause[\"type\"]\n","        if clause_type not in aggregated_clauses or clause[\"confidence\"] > aggregated_clauses[clause_type][\"confidence\"]:\n","            aggregated_clauses[clause_type] = clause\n","\n","    return list(aggregated_clauses.values())\n","\n","# ✅ Legal Document Analysis API\n","@app.post(\"/analyze_legal_document\")\n","async def analyze_legal_document(file: UploadFile = File(...)):\n","    \"\"\"Analyzes a legal document for clause detection and compliance risks.\"\"\"\n","    try:\n","        print(f\"Processing file: {file.filename}\")\n","        content = await file.read()\n","        text = extract_text_from_pdf(io.BytesIO(content))\n","\n","        if not text:\n","            return {\"status\": \"error\", \"message\": \"No valid text found in the document.\"}\n","\n","        # Truncate text for summarization if too long\n","        summary_text = text[:4096] if len(text) > 4096 else text\n","        summary = summarizer(summary_text, max_length=200, min_length=50, do_sample=False)[0]['summary_text'] if len(text) > 100 else \"Document too short for meaningful summarization.\"\n","\n","        print(\"Extracting named entities...\")\n","        entities = extract_named_entities(text)\n","\n","        print(\"Analyzing risk...\")\n","        risk_scores = analyze_risk(text)\n","        # Get detailed risk info including contextual summaries\n","        detailed_risk = get_detailed_risk_info(text)\n","\n","        print(\"Analyzing contract clauses...\")\n","        clauses = analyze_contract_clauses(text)\n","\n","        # ✅ Generate a unique Task ID (Properly Indented)\n","        generated_task_id = str(uuid.uuid4())\n","\n","        # ✅ Store document text for chatbot context\n","        store_document_context(generated_task_id, text)\n","\n","        return {  # ✅ Ensure proper indentation\n","            \"status\": \"success\",\n","            \"task_id\": generated_task_id,  # ✅ Ensure Task ID is included\n","            \"summary\": summary,\n","            \"named_entities\": entities,\n","            \"risk_scores\": risk_scores,\n","            \"detailed_risk\": detailed_risk,\n","            \"clauses_detected\": clauses\n","        }\n","\n","    except Exception as e:\n","        print(f\"Error processing document: {str(e)}\")\n","        return {\"status\": \"error\", \"message\": str(e)}\n","\n","# ✅ NEW: Legal Video Analysis API\n","@app.post(\"/analyze_legal_video\")\n","async def analyze_legal_video(file: UploadFile = File(...)):\n","    \"\"\"Analyzes a legal video by transcribing audio and analyzing the transcript.\"\"\"\n","    try:\n","        # Save the uploaded file to a temporary location\n","        print(f\"Processing video file: {file.filename}\")\n","        content = await file.read()\n","\n","        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp_file:\n","            temp_file.write(content)\n","            temp_file_path = temp_file.name\n","\n","        print(f\"Temporary file saved at: {temp_file_path}\")\n","\n","        # Extract text from video via audio transcription\n","        text = process_video_to_text(temp_file_path)\n","\n","        # Clean up temporary file\n","        if os.path.exists(temp_file_path):\n","            os.remove(temp_file_path)\n","\n","        if not text:\n","            return {\"status\": \"error\", \"message\": \"No speech could be transcribed from the video.\"}\n","\n","        # Store the transcript for reference\n","        transcript_path = os.path.join(\"static\", f\"transcript_{int(time.time())}.txt\")\n","        with open(transcript_path, \"w\") as f:\n","            f.write(text)\n","\n","        # Analyze the transcribed text (same as document analysis)\n","        summary_text = text[:4096] if len(text) > 4096 else text\n","        summary = summarizer(summary_text, max_length=200, min_length=50, do_sample=False)[0]['summary_text'] if len(text) > 100 else \"Transcript too short for meaningful summarization.\"\n","\n","        print(\"Extracting named entities from transcript...\")\n","        entities = extract_named_entities(text)\n","\n","        print(\"Analyzing risk from transcript...\")\n","        risk_scores = analyze_risk(text)\n","        detailed_risk = get_detailed_risk_info(text)\n","\n","        print(\"Analyzing legal clauses from transcript...\")\n","        clauses = analyze_contract_clauses(text)\n","\n","        # ✅ Generate a unique Task ID for video transcripts\n","        generated_task_id = str(uuid.uuid4())\n","\n","        # ✅ Store transcript text for chatbot context\n","        store_document_context(generated_task_id, text)\n","\n","        return {\n","            \"status\": \"success\",\n","            \"task_id\": generated_task_id,\n","            \"transcript\": text,\n","            \"transcript_path\": transcript_path,\n","            \"summary\": summary,\n","            \"named_entities\": entities,\n","            \"risk_scores\": risk_scores,\n","            \"detailed_risk\": detailed_risk,\n","            \"clauses_detected\": clauses\n","        }\n","    except Exception as e:\n","        print(f\"Error processing video: {str(e)}\")\n","        return {\"status\": \"error\", \"message\": str(e)}\n","\n","# ✅ NEW: Legal Audio Analysis API\n","@app.post(\"/analyze_legal_audio\")\n","async def analyze_legal_audio(file: UploadFile = File(...)):\n","    \"\"\"Analyzes legal audio by transcribing and analyzing the transcript.\"\"\"\n","    try:\n","        # Save the uploaded file to a temporary location\n","        print(f\"Processing audio file: {file.filename}\")\n","        content = await file.read()\n","\n","        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp_file:\n","            temp_file.write(content)\n","            temp_file_path = temp_file.name\n","\n","        print(f\"Temporary file saved at: {temp_file_path}\")\n","\n","        # Extract text from audio via transcription\n","        text = process_audio_to_text(temp_file_path)\n","\n","        # Clean up temporary file\n","        if os.path.exists(temp_file_path):\n","            os.remove(temp_file_path)\n","\n","        if not text:\n","            return {\"status\": \"error\", \"message\": \"No speech could be transcribed from the audio.\"}\n","\n","        # Store the transcript for reference\n","        transcript_path = os.path.join(\"static\", f\"transcript_{int(time.time())}.txt\")\n","        with open(transcript_path, \"w\") as f:\n","            f.write(text)\n","\n","        # Analyze the transcribed text (same as document analysis)\n","        summary_text = text[:4096] if len(text) > 4096 else text\n","        summary = summarizer(summary_text, max_length=200, min_length=50, do_sample=False)[0]['summary_text'] if len(text) > 100 else \"Transcript too short for meaningful summarization.\"\n","\n","        print(\"Extracting named entities from transcript...\")\n","        entities = extract_named_entities(text)\n","\n","        print(\"Analyzing risk from transcript...\")\n","        risk_scores = analyze_risk(text)\n","        detailed_risk = get_detailed_risk_info(text)\n","\n","        print(\"Analyzing legal clauses from transcript...\")\n","        clauses = analyze_contract_clauses(text)\n","\n","        # ✅ Generate a unique Task ID for audio transcripts\n","        generated_task_id = str(uuid.uuid4())\n","\n","        # ✅ Store transcript text for chatbot context\n","        store_document_context(generated_task_id, text)\n","\n","        return {\n","            \"status\": \"success\",\n","            \"task_id\": generated_task_id,\n","            \"transcript\": text,\n","            \"transcript_path\": transcript_path,\n","            \"summary\": summary,\n","            \"named_entities\": entities,\n","            \"risk_scores\": risk_scores,\n","            \"detailed_risk\": detailed_risk,\n","            \"clauses_detected\": clauses\n","        }\n","    except Exception as e:\n","        print(f\"Error processing audio: {str(e)}\")\n","        return {\"status\": \"error\", \"message\": str(e)}\n","\n","# ✅ Get Transcript API\n","@app.get(\"/transcript/{transcript_id}\")\n","async def get_transcript(transcript_id: str):\n","    \"\"\"Retrieves a previously generated transcript.\"\"\"\n","    transcript_path = os.path.join(\"static\", f\"transcript_{transcript_id}.txt\")\n","    if os.path.exists(transcript_path):\n","        return FileResponse(transcript_path)\n","    else:\n","        raise HTTPException(status_code=404, detail=\"Transcript not found\")\n","\n","# ✅ Fixed legal chatbot API\n","@app.post(\"/legal_chatbot\")\n","async def legal_chatbot_api(query: str = Form(...), task_id: str = Form(...)):\n","    \"\"\"Handles legal Q&A using chat history and document context.\"\"\"\n","\n","    # Retrieve the document text from storage\n","    document_context = load_document_context(task_id)\n","\n","    if not document_context:\n","        return {\"response\": \"⚠️ No relevant document found for this task ID.\"}\n","\n","    response = legal_chatbot(query, document_context)\n","\n","    return {\"response\": response, \"chat_history\": chat_history[-5:]}\n","\n","# ✅ Health Check Endpoint\n","@app.get(\"/health\")\n","async def health_check():\n","    return {\n","        \"status\": \"ok\",\n","        \"models_loaded\": True,\n","        \"device\": device,\n","        \"gpu_available\": torch.cuda.is_available(),\n","        \"timestamp\": time.time()\n","    }\n","\n","# ✅ Ngrok Setup for Google Colab\n","def setup_ngrok():\n","    \"\"\"Sets up ngrok tunnel for Google Colab.\"\"\"\n","    try:\n","        # Get auth token from environment or set it manually if you have one\n","        auth_token = os.environ.get(\"NGROK_AUTH_TOKEN\")\n","        if auth_token:\n","            ngrok.set_auth_token(auth_token)\n","\n","        ngrok.kill()  # Kill any existing tunnels\n","        time.sleep(1)  # Wait before reconnecting\n","\n","        # Connect to ngrok\n","        ngrok_tunnel = ngrok.connect(8500, \"http\")\n","        public_url = ngrok_tunnel.public_url\n","        print(f\"✅ Ngrok Public URL: {public_url}\")\n","\n","        # Keep ngrok connection alive\n","        def keep_alive():\n","            while True:\n","                time.sleep(60)\n","                try:\n","                    # Check tunnel status\n","                    tunnels = ngrok.get_tunnels()\n","                    if not tunnels:\n","                        print(\"⚠️ Ngrok tunnel closed. Reconnecting...\")\n","                        ngrok_tunnel = ngrok.connect(8500, \"http\")\n","                        print(f\"✅ Reconnected. New URL: {ngrok_tunnel.public_url}\")\n","                except Exception as e:\n","                    print(f\"⚠️ Ngrok error: {e}\")\n","\n","        Thread(target=keep_alive, daemon=True).start()\n","        return public_url\n","    except Exception as e:\n","        print(f\"⚠️ Ngrok setup error: {e}\")\n","        return None\n","\n","from fastapi.responses import FileResponse\n","\n","# ✅ Existing Risk Chart (Bar Chart) Endpoint\n","@app.get(\"/download_risk_chart\")\n","async def download_risk_chart():\n","    \"\"\"Generate and return a risk assessment chart as an image file.\"\"\"\n","    try:\n","        # Ensure the static directory exists\n","        os.makedirs(\"static\", exist_ok=True)\n","\n","        # Sample risk assessment data (should be dynamically fetched from API results)\n","        risk_scores = {\n","            \"Liability\": 11,\n","            \"Termination\": 12,\n","            \"Indemnification\": 10,\n","            \"Payment Risk\": 41,\n","            \"Insurance\": 71\n","        }\n","\n","        # Generate a bar chart for legal risk assessment\n","        plt.figure(figsize=(8, 5))\n","        plt.bar(risk_scores.keys(), risk_scores.values(), color='red')\n","        plt.xlabel(\"Risk Categories\")\n","        plt.ylabel(\"Risk Score\")\n","        plt.title(\"Legal Risk Assessment\")\n","        plt.xticks(rotation=30)\n","\n","        # Save the chart as an image file\n","        risk_chart_path = \"static/risk_chart.png\"\n","        plt.savefig(risk_chart_path)\n","        plt.close()\n","\n","        return FileResponse(risk_chart_path, media_type=\"image/png\", filename=\"risk_chart.png\")\n","\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=f\"Error generating risk chart: {str(e)}\")\n","\n","# ✅ Additional Visualization Endpoints\n","\n","@app.get(\"/download_risk_pie_chart\")\n","async def download_risk_pie_chart():\n","    try:\n","        # Sample risk assessment data\n","        risk_scores = {\n","            \"Liability\": 11,\n","            \"Termination\": 12,\n","            \"Indemnification\": 10,\n","            \"Payment Risk\": 41,\n","            \"Insurance\": 71\n","        }\n","\n","        # Generate a pie chart\n","        plt.figure(figsize=(6, 6))\n","        plt.pie(risk_scores.values(), labels=risk_scores.keys(), autopct='%1.1f%%', startangle=90)\n","        plt.title(\"Legal Risk Distribution\")\n","        pie_chart_path = \"static/risk_pie_chart.png\"\n","        plt.savefig(pie_chart_path)\n","        plt.close()\n","        return FileResponse(pie_chart_path, media_type=\"image/png\", filename=\"risk_pie_chart.png\")\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=f\"Error generating pie chart: {str(e)}\")\n","\n","@app.get(\"/download_risk_radar_chart\")\n","async def download_risk_radar_chart():\n","    try:\n","        risk_scores = {\n","            \"Liability\": 11,\n","            \"Termination\": 12,\n","            \"Indemnification\": 10,\n","            \"Payment Risk\": 41,\n","            \"Insurance\": 71\n","        }\n","        categories = list(risk_scores.keys())\n","        values = list(risk_scores.values())\n","\n","        # Radar chart requires the data to wrap around\n","        categories += categories[:1]\n","        values += values[:1]\n","\n","        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n","        angles += angles[:1]\n","\n","        fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n","        ax.plot(angles, values, 'o-', linewidth=2)\n","        ax.fill(angles, values, alpha=0.25)\n","        ax.set_thetagrids(np.degrees(angles[:-1]), categories)\n","        ax.set_title(\"Legal Risk Radar Chart\", y=1.1)\n","        radar_chart_path = \"static/risk_radar_chart.png\"\n","        plt.savefig(radar_chart_path)\n","        plt.close()\n","        return FileResponse(radar_chart_path, media_type=\"image/png\", filename=\"risk_radar_chart.png\")\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=f\"Error generating radar chart: {str(e)}\")\n","\n","@app.get(\"/download_risk_trend_chart\")\n","async def download_risk_trend_chart():\n","    try:\n","        # Sample historical risk scores for demonstration\n","        dates = [\"2025-01-01\", \"2025-02-01\", \"2025-03-01\", \"2025-04-01\"]\n","        risk_history = {\n","            \"Liability\": [10, 12, 11, 13],\n","            \"Termination\": [12, 15, 14, 13],\n","            \"Indemnification\": [9, 10, 11, 10],\n","            \"Payment Risk\": [40, 42, 41, 43],\n","            \"Insurance\": [70, 69, 71, 72]\n","        }\n","\n","        plt.figure(figsize=(10, 6))\n","        for category, scores in risk_history.items():\n","            plt.plot(dates, scores, marker='o', label=category)\n","        plt.xlabel(\"Date\")\n","        plt.ylabel(\"Risk Score\")\n","        plt.title(\"Historical Legal Risk Trends\")\n","        plt.xticks(rotation=45)\n","        plt.legend()\n","        trend_chart_path = \"static/risk_trend_chart.png\"\n","        plt.savefig(trend_chart_path, bbox_inches=\"tight\")\n","        plt.close()\n","        return FileResponse(trend_chart_path, media_type=\"image/png\", filename=\"risk_trend_chart.png\")\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=f\"Error generating trend chart: {str(e)}\")\n","\n","import pandas as pd\n","import plotly.express as px\n","from fastapi.responses import HTMLResponse\n","\n","@app.get(\"/interactive_risk_chart\", response_class=HTMLResponse)\n","async def interactive_risk_chart():\n","    try:\n","        risk_scores = {\n","            \"Liability\": 11,\n","            \"Termination\": 12,\n","            \"Indemnification\": 10,\n","            \"Payment Risk\": 41,\n","            \"Insurance\": 71\n","        }\n","        df = pd.DataFrame({\n","            \"Risk Category\": list(risk_scores.keys()),\n","            \"Risk Score\": list(risk_scores.values())\n","        })\n","        fig = px.bar(df, x=\"Risk Category\", y=\"Risk Score\", title=\"Interactive Legal Risk Assessment\")\n","        return fig.to_html()\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=f\"Error generating interactive chart: {str(e)}\")\n","\n","# ✅ Run FastAPI with proper Colab configuration\n","def run():\n","    \"\"\"Starts the FastAPI server.\"\"\"\n","    print(\"Starting FastAPI server...\")\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8500, timeout_keep_alive=600)\n","\n","if __name__ == \"__main__\":\n","    public_url = setup_ngrok()\n","    if public_url:\n","        print(f\"\\n✅ Your API is publicly available at: {public_url}/docs\\n\")\n","    else:\n","        print(\"\\n⚠️ Ngrok setup failed. API will only be available locally.\\n\")\n","\n","    run()\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"F2s5HBggzhsZ","executionInfo":{"status":"ok","timestamp":1741254360008,"user_tz":-330,"elapsed":150,"user":{"displayName":"Tejash Pandey","userId":"07192226250705781138"}}},"outputs":[],"source":["!fuser -k 8500/tcp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKX3yuN2zrcT","outputId":"8d5103a6-56ed-4102-9e83-c235dc6e90f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["error: XDG_RUNTIME_DIR not set in the environment.\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n","ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n","ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n","ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n","ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n","ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","2025-03-06 09:46:24.453280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1741254384.747522    1968 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1741254384.825600    1968 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-06 09:46:25.414911: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","✅ Loading NLP models...\n","config.json: 100% 1.33k/1.33k [00:00<00:00, 10.6MB/s]\n","pytorch_model.bin: 100% 2.28G/2.28G [00:12<00:00, 188MB/s]\n","model.safetensors:   5% 115M/2.28G [00:03<00:26, 80.5MB/s] \n","tokenizer_config.json: 100% 1.51k/1.51k [00:00<00:00, 6.00MB/s]\n","model.safetensors:   7% 168M/2.28G [00:03<00:17, 121MB/s] \n","spiece.model: 100% 1.91M/1.91M [00:00<00:00, 86.7MB/s]\n","model.safetensors:  13% 294M/2.28G [00:03<00:09, 208MB/s]\n","special_tokens_map.json: 100% 1.34k/1.34k [00:00<00:00, 6.16MB/s]\n","model.safetensors:  14% 325M/2.28G [00:04<00:12, 160MB/s]Device set to use cuda:0\n","model.safetensors:  23% 535M/2.28G [00:05<00:10, 162MB/s]\n","modules.json: 100% 349/349 [00:00<00:00, 2.12MB/s]\n","\n","config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 855kB/s]\n","model.safetensors:  24% 556M/2.28G [00:06<00:26, 64.2MB/s]\n","README.md: 100% 10.6k/10.6k [00:00<00:00, 47.2MB/s]\n","model.safetensors:  27% 608M/2.28G [00:07<00:15, 112MB/s] \n","sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 317kB/s]\n","model.safetensors:  29% 661M/2.28G [00:07<00:12, 130MB/s]\n","config.json: 100% 571/571 [00:00<00:00, 4.50MB/s]\n","model.safetensors:  48% 1.10G/2.28G [00:10<00:12, 94.1MB/s]\n","model.safetensors:  51% 1.15G/2.28G [00:11<00:08, 135MB/s]\n","model.safetensors:  51% 1.17G/2.28G [00:11<00:08, 134MB/s]\n","model.safetensors:  52% 1.20G/2.28G [00:11<00:07, 138MB/s]\n","model.safetensors:  54% 1.24G/2.28G [00:12<00:24, 43.1MB/s]\n","model.safetensors:  55% 1.26G/2.28G [00:14<00:35, 29.3MB/s]\n","model.safetensors:  56% 1.28G/2.28G [00:14<00:26, 37.7MB/s]\n","model.safetensors:  57% 1.30G/2.28G [00:14<00:19, 49.6MB/s]\n","model.safetensors:  58% 1.32G/2.28G [00:14<00:15, 63.6MB/s]\n","model.safetensors:  59% 1.34G/2.28G [00:14<00:12, 74.6MB/s]\n","model.safetensors:  60% 1.36G/2.28G [00:14<00:10, 88.4MB/s]\n","model.safetensors:  62% 1.41G/2.28G [00:15<00:07, 121MB/s]\n","model.safetensors:  62% 1.43G/2.28G [00:15<00:06, 133MB/s]\n","model.safetensors:  63% 1.45G/2.28G [00:15<00:06, 138MB/s]\n","model.safetensors:  57% 252M/438M [00:04<00:01, 120MB/s]\u001b[A\n","model.safetensors:  64% 1.47G/2.28G [00:15<00:05, 139MB/s]\n","model.safetensors:  65% 1.49G/2.28G [00:15<00:05, 144MB/s]\n","model.safetensors:  66% 1.51G/2.28G [00:15<00:05, 152MB/s]\n","model.safetensors:  77% 336M/438M [00:04<00:00, 134MB/s]\u001b[A\n","model.safetensors:  67% 1.54G/2.28G [00:16<00:06, 115MB/s]\n","model.safetensors:  89% 388M/438M [00:05<00:00, 172MB/s]\u001b[A\n","model.safetensors:  68% 1.56G/2.28G [00:16<00:06, 110MB/s]\n","model.safetensors: 100% 438M/438M [00:05<00:00, 78.3MB/s]\n","model.safetensors:  74% 1.69G/2.28G [00:17<00:03, 175MB/s]\n","tokenizer_config.json: 100% 363/363 [00:00<00:00, 1.56MB/s]\n","model.safetensors:  77% 1.75G/2.28G [00:17<00:02, 212MB/s]\n","vocab.txt: 100% 232k/232k [00:00<00:00, 3.32MB/s]\n","model.safetensors:  79% 1.81G/2.28G [00:17<00:02, 222MB/s]\n","model.safetensors:  81% 1.85G/2.28G [00:17<00:01, 236MB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 3.02MB/s]\n","model.safetensors:  86% 1.97G/2.28G [00:18<00:01, 239MB/s]\n","special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.20MB/s]\n","model.safetensors:  89% 2.03G/2.28G [00:18<00:01, 165MB/s]\n","config.json: 100% 190/190 [00:00<00:00, 1.17MB/s]\n","model.safetensors:  93% 2.13G/2.28G [00:19<00:00, 158MB/s]\n","config.json: 100% 829/829 [00:00<00:00, 5.33MB/s]\n","model.safetensors:  96% 2.20G/2.28G [00:19<00:00, 187MB/s]\n","model.safetensors:   0% 0.00/433M [00:00<?, ?B/s]\u001b[A\n","model.safetensors:  98% 2.23G/2.28G [00:19<00:00, 190MB/s]\n","model.safetensors: 100% 2.28G/2.28G [00:20<00:00, 150MB/s]\n","model.safetensors: 100% 2.28G/2.28G [00:20<00:00, 112MB/s]\n","\n","model.safetensors:  19% 83.9M/433M [00:00<00:02, 117MB/s]\u001b[A\n","model.safetensors:  27% 115M/433M [00:00<00:02, 151MB/s] \u001b[A\n","model.safetensors:  31% 136M/433M [00:00<00:01, 155MB/s]\u001b[A\n","model.safetensors:  39% 168M/433M [00:01<00:01, 183MB/s]\u001b[A\n","model.safetensors:  44% 189M/433M [00:01<00:01, 189MB/s]\u001b[A\n","model.safetensors:  48% 210M/433M [00:01<00:01, 184MB/s]\u001b[A\n","model.safetensors:  56% 241M/433M [00:01<00:00, 194MB/s]\u001b[A\n","model.safetensors:  61% 262M/433M [00:01<00:01, 132MB/s]\u001b[A\n","model.safetensors:  68% 294M/433M [00:01<00:00, 161MB/s]\u001b[A\n","model.safetensors:  75% 325M/433M [00:02<00:00, 178MB/s]\u001b[A\n","model.safetensors:  82% 357M/433M [00:02<00:00, 202MB/s]\u001b[A\n","model.safetensors:  90% 388M/433M [00:02<00:00, 216MB/s]\u001b[A\n","model.safetensors: 100% 433M/433M [00:02<00:00, 175MB/s]\n","Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tokenizer_config.json: 100% 59.0/59.0 [00:00<00:00, 602kB/s]\n","vocab.txt: 100% 213k/213k [00:00<00:00, 1.67MB/s]\n","added_tokens.json: 100% 2.00/2.00 [00:00<00:00, 18.2kB/s]\n","special_tokens_map.json: 100% 112/112 [00:00<00:00, 934kB/s]\n","Device set to use cuda:0\n","config.json: 100% 1.99k/1.99k [00:00<00:00, 18.5MB/s]\n","model.safetensors: 100% 3.06G/3.06G [00:15<00:00, 193MB/s]\n","generation_config.json: 100% 3.75k/3.75k [00:00<00:00, 30.3MB/s]\n","tokenizer_config.json: 100% 283k/283k [00:00<00:00, 46.0MB/s]\n","vocab.json: 100% 836k/836k [00:00<00:00, 11.3MB/s]\n","tokenizer.json: 100% 2.48M/2.48M [00:00<00:00, 8.19MB/s]\n","merges.txt: 100% 494k/494k [00:00<00:00, 1.85MB/s]\n","normalizer.json: 100% 52.7k/52.7k [00:00<00:00, 61.8MB/s]\n","added_tokens.json: 100% 34.6k/34.6k [00:00<00:00, 142MB/s]\n","special_tokens_map.json: 100% 2.19k/2.19k [00:00<00:00, 20.1MB/s]\n","preprocessor_config.json: 100% 185k/185k [00:00<00:00, 122MB/s]\n","Device set to use cuda:0\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 436kB/s]\n","config.json: 100% 1.02k/1.02k [00:00<00:00, 6.18MB/s]\n","vocab.txt: 100% 222k/222k [00:00<00:00, 3.59MB/s]\n","special_tokens_map.json: 100% 112/112 [00:00<00:00, 1.01MB/s]\n","pytorch_model.bin: 100% 440M/440M [00:01<00:00, 233MB/s]\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","✅ All models loaded successfully\n","model.safetensors:   0% 0.00/440M [00:00<?, ?B/s]\n","config.json: 100% 571/571 [00:00<00:00, 5.07MB/s]\n","model.safetensors:  10% 41.9M/440M [00:00<00:03, 129MB/s] \n","model.safetensors:  17% 73.4M/440M [00:00<00:02, 180MB/s]\n","model.safetensors:  29% 126M/440M [00:00<00:01, 193MB/s] \n","model.safetensors:  36% 157M/440M [00:00<00:01, 193MB/s]\n","model.safetensors:  40% 178M/440M [00:01<00:01, 178MB/s]\n","model.safetensors:  45% 199M/440M [00:01<00:01, 176MB/s]\n","model.safetensors:  50% 220M/440M [00:01<00:01, 177MB/s]\n","model.safetensors:  55% 241M/440M [00:01<00:01, 176MB/s]\n","model.safetensors:  60% 262M/440M [00:01<00:01, 170MB/s]\n","model.safetensors:  64% 283M/440M [00:01<00:00, 178MB/s]\n","model.safetensors:  69% 304M/440M [00:01<00:00, 184MB/s]\n","model.safetensors:  74% 325M/440M [00:01<00:00, 158MB/s]\n","model.safetensors:  79% 346M/440M [00:02<00:00, 149MB/s]\n","model.safetensors:  83% 367M/440M [00:02<00:00, 116MB/s]\n","model.safetensors:  88% 388M/440M [00:02<00:00, 105MB/s]\n","model.safetensors:  93% 409M/440M [00:02<00:00, 86.9MB/s]\n","model.safetensors:  95% 419M/440M [00:03<00:00, 82.4MB/s]\n","model.safetensors: 100% 440M/440M [00:03<00:00, 45.5MB/s]\n","model.safetensors: 100% 440M/440M [00:03<00:00, 114MB/s] \n","\n","model.safetensors:  72% 357M/496M [00:03<00:02, 62.9MB/s]\u001b[A\n","model.safetensors:  76% 377M/496M [00:03<00:01, 81.4MB/s]\u001b[A\n","model.safetensors:  80% 398M/496M [00:03<00:01, 86.0MB/s]\u001b[A\n","model.safetensors:  85% 419M/496M [00:04<00:00, 86.1MB/s]\u001b[A\n","model.safetensors:  89% 440M/496M [00:04<00:00, 87.6MB/s]\u001b[A\n","model.safetensors:  93% 461M/496M [00:04<00:00, 97.2MB/s]\u001b[A\n","model.safetensors: 100% 496M/496M [00:04<00:00, 105MB/s]\n","tokenizer_config.json: 100% 79.0/79.0 [00:00<00:00, 664kB/s]\n","vocab.json: 100% 899k/899k [00:00<00:00, 8.04MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 99.1MB/s]\n","special_tokens_map.json: 100% 772/772 [00:00<00:00, 7.33MB/s]\n","Device set to use cuda:0\n","✅ Ngrok Public URL: https://1963-35-185-253-183.ngrok-free.app\n","\n","✅ Your API is publicly available at: https://1963-35-185-253-183.ngrok-free.app/docs\n","\n","Starting FastAPI server...\n","\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1968\u001b[0m]\n","\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n","\u001b[32mINFO\u001b[0m:     Application startup complete.\n","\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8500\u001b[0m (Press CTRL+C to quit)\n","Processing file: tmpmgk346y0.pdf\n","Extracting named entities...\n","Analyzing risk...\n","Analyzing contract clauses...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_document HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mGET /download_risk_chart HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","Processing file: tmpv42she5t.pdf\n","Extracting named entities...\n","Analyzing risk...\n","Analyzing contract clauses...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_document HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mGET /download_risk_pie_chart HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /legal_chatbot HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","Processing file: tmp0ek8xjs9.pdf\n","Extracting named entities...\n","Analyzing risk...\n","Analyzing contract clauses...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_document HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mGET /download_risk_chart HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /legal_chatbot HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /legal_chatbot HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /legal_chatbot HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","Processing file: tmp_rqh88k0.pdf\n","Extracting named entities...\n","Analyzing risk...\n","Analyzing contract clauses...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_document HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mGET /download_risk_chart HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","Processing file: tmpg2vyw8vs.pdf\n","Extracting named entities...\n","Analyzing risk...\n","Analyzing contract clauses...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_document HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mGET /download_risk_pie_chart HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","Processing file: tmphnqjxdlx.pdf\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","Extracting named entities...\n","Analyzing risk...\n","Analyzing contract clauses...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_document HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mGET /download_risk_chart HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","Processing file: tmp1zb6t7yx.pdf\n","Extracting named entities...\n","Analyzing risk...\n","Analyzing contract clauses...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_document HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mGET /download_risk_pie_chart HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","Processing file: tmpg5_upjct.pdf\n","Extracting named entities...\n","Analyzing risk...\n","Analyzing contract clauses...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_document HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mGET /download_risk_trend_chart HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","Processing video file: tmpt52det53.mp4\n","Temporary file saved at: /tmp/tmp6f03_5pi.mp4\n","Processing video file at /tmp/tmp6f03_5pi.mp4\n","MoviePy - Writing audio in temp/extracted_audio.wav\n","MoviePy - Done.\n","Audio extracted to temp/extracted_audio.wav\n","Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n","Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Transcription completed: 2126 characters\n","Extracting named entities from transcript...\n","Analyzing risk from transcript...\n","Your max_length is set to 100, but your input_length is only 30. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n","Your max_length is set to 100, but your input_length is only 72. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n","Analyzing legal clauses from transcript...\n","\u001b[32mINFO\u001b[0m:     35.237.93.237:0 - \"\u001b[1mPOST /analyze_legal_video HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"]}],"source":["!python app.py"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPc00uWd6rrPC26xVa6ID7Q"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}